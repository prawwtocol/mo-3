## Метод стохастического градиентного спуска (SGD) и его модификации

**Основной язык/среда выполнения лабораторных работ:** Python 3, Jupyter notebook, Google Colab. При желании можно экспериментировать на других языках: Julia, Mojo, С++ и т.д. 

Лабораторные работы выполняются индивидуально или в группах по 2-3-4 человека. По результатам выполнения лабораторной работы необходимо подготовить отчёт. Отчёт должен содержать постановку задачи, описание используемых методов, (ссылку на) реализацию, результаты исследования (графики и таблицы, которые их демонстрируют, анализ результатов, преимуществ и ограничений методов). 

*Так в этом тексте выделяются библиотеки, так то, что полезно для следующих лабораторных.*

Лабораторная состоит из основного и двух дополнительных заданий.

## Основное задание

### Разработка
Реализуйте и исследуйте на эффективность SGD для решения полиномиальной или многомерной линейной регрессии:

1. с разным размером батча – от одного до размера полной коллекции (обычный GD)
2. с разной функцией изменения шага (learning rate scheduling) и регуляризацией (L1, L2, Elastic);
3. `keras.optimizers`, `torch.optim`: SGD[src](src), и модификации SGD (Nesterov, Momentum, AdaGrad, RMSProp, Adam). Изучите параметры вызываемых библиотечных функций.

### Исследование

**[Объект исследования]** Найдите датасет, который Вам интересен. Можно взять датасет в архиве https://archive.ics.uci.edu/ или из любых других ресурсов. Также, можно и сгенерировать данные каким-либо образом (и, тем самым, оценивать, насколько хорошо метод решает задачу восстановления регрессии)

**[Суть исследования]**
1. Сравните эффективность SGD с разным размером батча по точности, скорости и ресурсам: объёму требуемой оперативной памяти и количеству арифметических операций.
2. Сравните эффективность SGD с разным гиперпараметрами, методами регуляризации. Cравните разные модификации SGD. Подбирать гиперпараметры можно также с помощью `optuna`.

## Дополнительное задание 1 (дополнительный метод)

Сделайте свою реализацию какого-либо метода модификации SGD или разберите стороннюю библиотечную реализацию данного метода, проанализируйте особенности реализации, модифицируйте и интегрируйте код метода в свой проект. Исследуйте его на эффективность в сравнении с другими методами.

## Дополнительное задание 2 (дополнительная задача)

Разберите подробней постановку задачи оптимизации в наиболее интересной для Вас задачи машинного обучения (метод опорных векторов, активное обучение, бустинг и пр.). Приведите пример, иллюстрирующий задачу и её решение (найдите на ресурсах).

## Оформление

Отчёт должен содержать всю необходимую информацию, но быть компактным

**[Описание методов]** В начале отчёта опишите используемые методы. В описании метода укажите, это реализованный Вами или библиотечный метод, из какой библиотеки он взят, или какую библиотеку использует внутри реализации, если использует. При наличии, указывайте важные особенности Вашей реализации метода, они могут быть алгоритмические (например, имеющиеся hyperпараметры), так и технические (например, использование мемоизации в алгоритме).

**[Описание результатов]**

В результатах работы метода должна быть следующая информация:
- объект исследования (функция) к которой применяется метод, начальная точка;
- особенности применения метода: стратегия выбора шага, значение гиперпараметров, критерий остановки;
- результат, количество итераций, количество вычислений значений функции и её производной (градиента);

## Критерии оценивания

1. Работоспособность и качество кода.
2. Полнота отчета и его компактность. В отчёте должны присутствовать: постановка задачи, описания используемых методов, результаты исследования, графики и таблицы, которые их демонстрируют, анализ результатов, преимущества и ограничения методов.
3. Знание теории, которая лежит в основе применяемых методов.
4. Дополнительное задание 1.
5. Дополнительное задание 2.

Каждый критерий оценивается максимально в 5 баллов.

**Максимальный балл за лабораторную работу составляет 25 баллов.**